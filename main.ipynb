{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "751d8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68b2f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sarcasm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2746ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_60</td>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>I never would have identified the fingerprints...</td>\n",
       "      <td>LEONARD|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_70</td>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>This is one of my favorite places to kick back...</td>\n",
       "      <td>HOWARD|PENNY|HOWARD|HOWARD|HOWARD|PENNY|HOWARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_80</td>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>Here we go. Pad thai, no peanuts.|But does it ...</td>\n",
       "      <td>LEONARD|HOWARD|LEONARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_90</td>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>A marathon? How many Superman movies are there...</td>\n",
       "      <td>PENNY|SHELDON|PENNY|SHELDON|SHELDON|PENNY|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_105</td>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>Great Caesar's ghost, look at this place.|So P...</td>\n",
       "      <td>SHELDON|LEONARD|SHELDON|SHELDON|SHELDON|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          utterance  speaker  \\\n",
       "0   1_60  It's just a privilege to watch your mind at work.  SHELDON   \n",
       "1   1_70  I don't think I'll be able to stop thinking ab...    PENNY   \n",
       "2   1_80  Since it's not bee season, you can have my epi...  SHELDON   \n",
       "3   1_90  Lois Lane is falling, accelerating at an initi...  SHELDON   \n",
       "4  1_105  I'm just inferring this is a couch because the...  SHELDON   \n",
       "\n",
       "                                             context  \\\n",
       "0  I never would have identified the fingerprints...   \n",
       "1  This is one of my favorite places to kick back...   \n",
       "2  Here we go. Pad thai, no peanuts.|But does it ...   \n",
       "3  A marathon? How many Superman movies are there...   \n",
       "4  Great Caesar's ghost, look at this place.|So P...   \n",
       "\n",
       "                                    context_speakers show  sarcasm  \n",
       "0                                    LEONARD|SHELDON  BBT     True  \n",
       "1     HOWARD|PENNY|HOWARD|HOWARD|HOWARD|PENNY|HOWARD  BBT     True  \n",
       "2                             LEONARD|HOWARD|LEONARD  BBT    False  \n",
       "3  PENNY|SHELDON|PENNY|SHELDON|SHELDON|PENNY|SHELDON  BBT    False  \n",
       "4    SHELDON|LEONARD|SHELDON|SHELDON|SHELDON|SHELDON  BBT     True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12d4c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Columns -> Utterances & Speaker\n",
    "def clean_text_cols12(text):\n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "   \n",
    "    # Remove all punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove Apostrophe & Full-Stop\n",
    "    for word in text:\n",
    "        if \"'\" in word:\n",
    "            word = word.replace(\"'\", \"\")\n",
    "        if \".\" in word:\n",
    "            word = word.replace(\".\", \"\")\n",
    "    \n",
    "    # Remove all digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "   \n",
    "    # Remove all extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "   \n",
    "    # Remove all stop words (optional)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "   \n",
    "    return text\n",
    "\n",
    "# | Stop Word | Punctuation | Lowercase | \n",
    "df['utterance'] = df['utterance'].apply(clean_text_cols12)\n",
    "df['speaker'] = df['speaker'].apply(clean_text_cols12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c813e003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_60</td>\n",
       "      <td>privilege watch mind work</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>I never would have identified the fingerprints...</td>\n",
       "      <td>LEONARD|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_70</td>\n",
       "      <td>dont think ill able stop thinking</td>\n",
       "      <td>penny</td>\n",
       "      <td>This is one of my favorite places to kick back...</td>\n",
       "      <td>HOWARD|PENNY|HOWARD|HOWARD|HOWARD|PENNY|HOWARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_80</td>\n",
       "      <td>since bee season epinephrine</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>Here we go. Pad thai, no peanuts.|But does it ...</td>\n",
       "      <td>LEONARD|HOWARD|LEONARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_90</td>\n",
       "      <td>lois lane falling accelerating initial rate fe...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>A marathon? How many Superman movies are there...</td>\n",
       "      <td>PENNY|SHELDON|PENNY|SHELDON|SHELDON|PENNY|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_105</td>\n",
       "      <td>im inferring couch evidence suggests coffee ta...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>Great Caesar's ghost, look at this place.|So P...</td>\n",
       "      <td>SHELDON|LEONARD|SHELDON|SHELDON|SHELDON|SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          utterance  speaker  \\\n",
       "0   1_60                          privilege watch mind work  sheldon   \n",
       "1   1_70                  dont think ill able stop thinking    penny   \n",
       "2   1_80                       since bee season epinephrine  sheldon   \n",
       "3   1_90  lois lane falling accelerating initial rate fe...  sheldon   \n",
       "4  1_105  im inferring couch evidence suggests coffee ta...  sheldon   \n",
       "\n",
       "                                             context  \\\n",
       "0  I never would have identified the fingerprints...   \n",
       "1  This is one of my favorite places to kick back...   \n",
       "2  Here we go. Pad thai, no peanuts.|But does it ...   \n",
       "3  A marathon? How many Superman movies are there...   \n",
       "4  Great Caesar's ghost, look at this place.|So P...   \n",
       "\n",
       "                                    context_speakers show  sarcasm  \n",
       "0                                    LEONARD|SHELDON  BBT     True  \n",
       "1     HOWARD|PENNY|HOWARD|HOWARD|HOWARD|PENNY|HOWARD  BBT     True  \n",
       "2                             LEONARD|HOWARD|LEONARD  BBT    False  \n",
       "3  PENNY|SHELDON|PENNY|SHELDON|SHELDON|PENNY|SHELDON  BBT    False  \n",
       "4    SHELDON|LEONARD|SHELDON|SHELDON|SHELDON|SHELDON  BBT     True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d78bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3481291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['utterance'] = df['utterance'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Only for column -> context_speakers\n",
    "def tokenize_context_speakers(value):\n",
    "    # To Lower Case\n",
    "    value = value.lower()\n",
    "   \n",
    "    # Split by '|''\n",
    "    tokenized_value = value.split(\"|\")\n",
    "    return tokenized_value\n",
    "   \n",
    "df['context_speakers'] = df['context_speakers'].apply(tokenize_context_speakers)\n",
    "\n",
    "# Only for column -> context_speakers\n",
    "def tokenize_context(value):\n",
    "    # To Lower Case\n",
    "    value = value.lower()\n",
    "    # Split by '|''\n",
    "    tokenized_value = value.split(\"|\")\n",
    "    return tokenized_value\n",
    "   \n",
    "df['context'] = df['context'].apply(tokenize_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11b786bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(value):\n",
    "    if value == False:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['sarcasm'] = df['sarcasm'].apply(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c513ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform PRODUCT_TYPE_ID column\n",
    "df['show'] = le.fit_transform(df['show'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55ab91f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_60</td>\n",
       "      <td>privilege watch mind work</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[i never would have identified the fingerprint...</td>\n",
       "      <td>[leonard, sheldon]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_70</td>\n",
       "      <td>dont think ill able stop thinking</td>\n",
       "      <td>penny</td>\n",
       "      <td>[this is one of my favorite places to kick bac...</td>\n",
       "      <td>[howard, penny, howard, howard, howard, penny,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_80</td>\n",
       "      <td>since bee season epinephrine</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[here we go. pad thai, no peanuts., but does i...</td>\n",
       "      <td>[leonard, howard, leonard]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_90</td>\n",
       "      <td>lois lane falling accelerating initial rate fe...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[a marathon? how many superman movies are ther...</td>\n",
       "      <td>[penny, sheldon, penny, sheldon, sheldon, penn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_105</td>\n",
       "      <td>im inferring couch evidence suggests coffee ta...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[great caesar's ghost, look at this place., so...</td>\n",
       "      <td>[sheldon, leonard, sheldon, sheldon, sheldon, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          utterance  speaker  \\\n",
       "0   1_60                          privilege watch mind work  sheldon   \n",
       "1   1_70                  dont think ill able stop thinking    penny   \n",
       "2   1_80                       since bee season epinephrine  sheldon   \n",
       "3   1_90  lois lane falling accelerating initial rate fe...  sheldon   \n",
       "4  1_105  im inferring couch evidence suggests coffee ta...  sheldon   \n",
       "\n",
       "                                             context  \\\n",
       "0  [i never would have identified the fingerprint...   \n",
       "1  [this is one of my favorite places to kick bac...   \n",
       "2  [here we go. pad thai, no peanuts., but does i...   \n",
       "3  [a marathon? how many superman movies are ther...   \n",
       "4  [great caesar's ghost, look at this place., so...   \n",
       "\n",
       "                                    context_speakers  show  sarcasm  \n",
       "0                                 [leonard, sheldon]     0        1  \n",
       "1  [howard, penny, howard, howard, howard, penny,...     0        1  \n",
       "2                         [leonard, howard, leonard]     0        0  \n",
       "3  [penny, sheldon, penny, sheldon, sheldon, penn...     0        0  \n",
       "4  [sheldon, leonard, sheldon, sheldon, sheldon, ...     0        1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de0d6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "# # initialize Porter stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # define a function to apply stemming to a text\n",
    "# def apply_stemming(words):\n",
    "#     # apply stemming to each word\n",
    "#     stemmed_words = [stemmer.stem(word) for word in words]\n",
    "#     # join stemmed words into a single string\n",
    "#     stemmed_text = \" \".join(stemmed_words)\n",
    "#     return stemmed_text\n",
    "\n",
    "# df['utterance'] = df['utterance'].apply(apply_stemming)\n",
    "# df['context'] = df['context'].apply(apply_stemming)\n",
    "\n",
    "# df['utterance'] = df['utterance'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f468aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define function to map POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# define function to perform lemmatization on a sentence of CONTEXT\n",
    "def lemmatization(cell):\n",
    "    \n",
    "    lemma_result = []\n",
    "    \n",
    "    for sentence in cell:\n",
    "        \n",
    "        # tokenize each sentence in cell\n",
    "        tokens = sentence.split(\" \")\n",
    "        \n",
    "        # lemmatize each tokens\n",
    "        lemmatized_list_of_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "   \n",
    "        # append to lemma_result -> list of list\n",
    "        lemma_result.append(lemmatized_list_of_tokens)\n",
    "   \n",
    "    return lemma_result\n",
    "\n",
    "# define function to perform lemmatization on a sentence of UTTERANCES\n",
    "def lemmatization_col1(sentence):\n",
    "\n",
    "    # tokenize each sentence in cell\n",
    "    tokens = sentence.split(\" \")\n",
    "\n",
    "    # lemmatize each tokens\n",
    "    lemmatized_list_of_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "   \n",
    "    return lemmatized_list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f8e13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization on column -> context\n",
    "df['context'] = df['context'].apply(lemmatization)\n",
    "\n",
    "# Apply lemmatization on column -> utterance\n",
    "df['utterance'] = df['utterance'].apply(lemmatization_col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c7a2065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_60</td>\n",
       "      <td>[privilege, watch, mind, work]</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[[i, never, would, have, identify, the, finger...</td>\n",
       "      <td>[leonard, sheldon]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_70</td>\n",
       "      <td>[dont, think, ill, able, stop, think]</td>\n",
       "      <td>penny</td>\n",
       "      <td>[[this, be, one, of, my, favorite, place, to, ...</td>\n",
       "      <td>[howard, penny, howard, howard, howard, penny,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_80</td>\n",
       "      <td>[since, bee, season, epinephrine]</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[[here, we, go., pad, thai,, no, peanuts.], [b...</td>\n",
       "      <td>[leonard, howard, leonard]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_90</td>\n",
       "      <td>[lois, lane, fall, accelerate, initial, rate, ...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[[a, marathon?, how, many, superman, movie, be...</td>\n",
       "      <td>[penny, sheldon, penny, sheldon, sheldon, penn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_105</td>\n",
       "      <td>[im, infer, couch, evidence, suggests, coffee,...</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>[[great, caesar's, ghost,, look, at, this, pla...</td>\n",
       "      <td>[sheldon, leonard, sheldon, sheldon, sheldon, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          utterance  speaker  \\\n",
       "0   1_60                     [privilege, watch, mind, work]  sheldon   \n",
       "1   1_70              [dont, think, ill, able, stop, think]    penny   \n",
       "2   1_80                  [since, bee, season, epinephrine]  sheldon   \n",
       "3   1_90  [lois, lane, fall, accelerate, initial, rate, ...  sheldon   \n",
       "4  1_105  [im, infer, couch, evidence, suggests, coffee,...  sheldon   \n",
       "\n",
       "                                             context  \\\n",
       "0  [[i, never, would, have, identify, the, finger...   \n",
       "1  [[this, be, one, of, my, favorite, place, to, ...   \n",
       "2  [[here, we, go., pad, thai,, no, peanuts.], [b...   \n",
       "3  [[a, marathon?, how, many, superman, movie, be...   \n",
       "4  [[great, caesar's, ghost,, look, at, this, pla...   \n",
       "\n",
       "                                    context_speakers  show  sarcasm  \n",
       "0                                 [leonard, sheldon]     0        1  \n",
       "1  [howard, penny, howard, howard, howard, penny,...     0        1  \n",
       "2                         [leonard, howard, leonard]     0        0  \n",
       "3  [penny, sheldon, penny, sheldon, sheldon, penn...     0        0  \n",
       "4  [sheldon, leonard, sheldon, sheldon, sheldon, ...     0        1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "052ac083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 613.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
      "     ------------------------------------ 224.2/224.2 kB 807.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jia\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 889.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jia\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jia\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9364e44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9152a7cfd3a2445887940d69fb894ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jia\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fe05b3ac4f4a4293688bbbe722afc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33e9dfd845e40da9ced8872b3631a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc8e67d6a274471b312654bc097578c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define custom dataset class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        utterance = self.data['utterance'][index]\n",
    "        label = self.data[[0, 1]].iloc[index].tolist()\n",
    "        \n",
    "        # Tokenize input text\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            utterance,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            'token_type_ids': inputs['token_type_ids'][0],\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Define data loaders\n",
    "train_dataset = SarcasmDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = SarcasmDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define BERT model and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        label = batch['label']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=label)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Loss: {loss.item()}')\n",
    "    \n",
    "# Evaluate model\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    token_type_ids = batch['token_type_ids']\n",
    "    label = batch['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = torch.sum(preds == torch.argmax(label, dim=1))\n",
    "    \n",
    "    total_correct += correct.item()\n",
    "    total_samples += len(preds)\n",
    "    \n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7018b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
